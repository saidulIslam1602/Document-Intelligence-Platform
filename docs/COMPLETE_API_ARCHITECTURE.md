# Complete API Architecture & Communication Flow
**Document Intelligence Platform - Comprehensive Endpoint & Integration Analysis**

Generated: December 28, 2025  
Total Services: 14 microservices  
Total Endpoints: 150+  
Total Code Lines: 11,629 lines (main.py files only)

---

## ğŸ—ï¸ Architecture Overview

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         EXTERNAL CLIENTS                                â”‚
â”‚  React Frontend (port 3001) â”‚ Mobile Apps â”‚ Claude Desktop â”‚ API Keys   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚ HTTPS
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    API GATEWAY (port 8003)                              â”‚
â”‚  â€¢ JWT Authentication â€¢ Rate Limiting â€¢ Circuit Breaker                 â”‚
â”‚  â€¢ Request Routing â€¢ Load Balancing â€¢ CORS                              â”‚
â””â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚
    â†“         â†“         â†“         â†“         â†“         â†“         â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚Documentâ”‚â”‚AI Proc-â”‚â”‚Analyti-â”‚â”‚AI Chat â”‚â”‚MCP     â”‚â”‚Data    â”‚â”‚Batch   â”‚
â”‚Ingest  â”‚â”‚essing  â”‚â”‚cs      â”‚â”‚        â”‚â”‚Server  â”‚â”‚Quality â”‚â”‚Process â”‚
â”‚:8000   â”‚â”‚:8001   â”‚â”‚:8002   â”‚â”‚:8004   â”‚â”‚:8012   â”‚â”‚:8006   â”‚â”‚:8007   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜â””â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â”‚         â”‚         â”‚         â”‚         â”‚         â”‚         â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                  â†“
         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
         â”‚      SHARED INFRASTRUCTURE                â”‚
         â”‚  PostgreSQL DB â”‚ Redis Cache â”‚ Storage   â”‚
         â”‚  :5432         â”‚ :6379       â”‚ (Blob)    â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“¡ Frontend â†’ Backend Communication

### Frontend Configuration
**File:** `frontend/src/services/api.ts`

```typescript
const API_URL = 'http://localhost:8003'  // API Gateway

export const api = axios.create({
  baseURL: API_URL,
  timeout: 30000,
})

// Automatic JWT token injection
api.interceptors.request.use((config) => {
  const token = localStorage.getItem('token');
  if (token) {
    config.headers.Authorization = `Bearer ${token}`;
  }
  return config;
});
```

### Frontend API Calls by Page

| Frontend Page | Endpoint Called | Method | Purpose |
|--------------|----------------|--------|---------|
| **Documents.tsx** | `/documents` | GET | List documents |
| | `/documents/{id}` | GET | Get document details |
| | `/documents/{id}` | DELETE | Delete document |
| **Dashboard.tsx** | `/analytics/automation-metrics` | GET | Automation metrics |
| | `/documents?limit=5` | GET | Recent documents |
| **Chat.tsx** | `/chat/message` | POST | Send chat message |
| **Entities.tsx** | `/entities` | GET | List extracted entities |
| **ProcessingPipeline.tsx** | `/processing/jobs?limit=10` | GET | Processing jobs |
| **MCPTools.tsx** | `/mcp/tools` | GET | List MCP tools |
| | `/mcp/execute-tool` | POST | Execute MCP tool |
| **Login.tsx** | `/auth/login` | POST | User authentication |
| **Analytics.tsx** | `/analytics/automation-metrics` | GET | Analytics data |
| **Admin.tsx** | `/admin/health` | GET | System health |
| | `/admin/users` | GET | User management |
| | `/admin/logs?limit=10` | GET | System logs |
| **AuditLogs.tsx** | `/audit/logs` | GET | Audit trail |
| **Search.tsx** | `/search?{params}` | GET | Search documents |

---

## ğŸšª API Gateway Endpoints (Port 8003)

**Service:** `src/microservices/api-gateway/main.py` (2,301 lines)

### Health & Monitoring
```
GET  /health                    â†’ System health check
GET  /health/live               â†’ Liveness probe
GET  /health/ready              â†’ Readiness probe
GET  /circuit-breakers          â†’ Circuit breaker status
POST /circuit-breakers/{name}/reset â†’ Reset breaker
POST /circuit-breakers/reset-all â†’ Reset all breakers
GET  /rate-limiters             â†’ Rate limiter status
POST /rate-limiters/{name}/reset â†’ Reset limiter
POST /rate-limiters/reset-all   â†’ Reset all limiters
GET  /rate-limit                â†’ Check rate limit
```

### Authentication & Authorization
```
POST /auth/login                â†’ User login (JWT token)
POST /auth/register             â†’ User registration
POST /auth/refresh              â†’ Refresh JWT token
POST /auth/logout               â†’ User logout
GET  /auth/me                   â†’ Get current user
POST /api-keys                  â†’ Create API key
GET  /api-keys                  â†’ List API keys
DELETE /api-keys/{id}           â†’ Revoke API key
```

### Document Management
```
GET  /documents                 â†’ List documents
GET  /documents/{id}            â†’ Get document details
POST /documents/upload          â†’ Upload document (forwards to document-ingestion:8000)
DELETE /documents/{id}          â†’ Delete document (forwards to document-ingestion:8000)
GET  /entities                  â†’ Get extracted entities
```

### Analytics
```
GET /analytics/automation-metrics â†’ Automation metrics (forwards to analytics:8002)
```

### AI Chat
```
POST /chat/message              â†’ Send chat message (forwards to ai-chat:8004)
```

### Service Routing Map
```python
SERVICE_URLS = {
    "document-ingestion": "http://document-ingestion:8000",
    "ai-processing": "http://ai-processing:8001",
    "analytics": "http://analytics:8002",
    "ai-chat": "http://ai-chat:8004",
    "performance-dashboard": "http://performance-dashboard:8005",
    "data-quality": "http://data-quality:8006",
    "batch-processor": "http://batch-processor:8007",
    "data-catalog": "http://data-catalog:8008",
    "migration-service": "http://migration-service:8009",
    "fabric-integration": "http://fabric-integration:8010",
    "demo-service": "http://demo-service:8011",
    "mcp-server": "http://mcp-server:8012"
}
```

---

## ğŸ“„ Document Ingestion Service (Port 8000)

**Service:** `src/microservices/document-ingestion/main.py` (1,321 lines)

### Endpoints
```
GET  /health                    â†’ Health check
POST /documents/upload          â†’ Upload single document
POST /documents/batch-upload    â†’ Upload 10-15 documents
GET  /documents/{id}/status     â†’ Get processing status
GET  /documents                 â†’ List user documents
DELETE /documents/{id}          â†’ Delete document
```

### Processing Flow
```
1. Client uploads file â†’ API Gateway :8003
2. API Gateway forwards â†’ Document Ingestion :8000
3. Document Ingestion:
   - Validates file (size, type)
   - Uploads to blob storage (Azure/Local)
   - Stores metadata in PostgreSQL
   - Creates processing job
   - Publishes event to Event Hub
   - Calls AI Processing :8001 via HTTP
4. Returns document_id to client
```

### Key Operations
```python
# Store document in PostgreSQL
sql_service.store_document({
    "document_id": document_id,
    "user_id": user_id,
    "file_name": file.filename,
    "file_size": file_size,
    "status": "uploaded"
})

# Schedule AI processing
async with httpx.AsyncClient() as client:
    response = await client.post(
        "http://ai-processing:8001/process",
        json={"document_id": document_id, "user_id": user_id}
    )
```

---

## ğŸ¤– AI Processing Service (Port 8001)

**Service:** `src/microservices/ai-processing/main.py` (1,500+ lines)

### Endpoints
```
GET  /health                    â†’ Health check
POST /process                   â†’ Process document with AI
POST /batch-process             â†’ Batch processing
POST /classify                  â†’ Document classification
POST /sentiment                 â†’ Sentiment analysis
POST /qa                        â†’ Question answering
POST /entities                  â†’ Named entity recognition
POST /summarize                 â†’ Text summarization
POST /models/train              â†’ Train custom model
POST /process-invoice-langchain â†’ LangChain invoice processing
POST /analyze-document-langchain â†’ LangChain document analysis
POST /fine-tuning-workflow-langchain â†’ Fine-tuning workflow
POST /process-document-agent    â†’ Agent-based processing
POST /process-intelligent       â†’ Intelligent routing
GET  /routing/statistics        â†’ Routing statistics
POST /llmops/track-model-metrics â†’ Track model metrics
POST /llmops/compare-models     â†’ Compare models
POST /llmops/optimize-for-goal  â†’ Optimize for goal
GET  /llmops/automation-dashboard â†’ LLMOps dashboard
GET  /models/status             â†’ Model status
```

### AI Processing Flow
```
1. Receives document_id from document-ingestion
2. Retrieves document from PostgreSQL/Storage
3. Extracts text using OCR (if PDF/image)
4. Routes to appropriate AI model:
   - OpenAI GPT-4 for invoices
   - Azure Form Recognizer for structured docs
   - Custom models for specific types
5. Extracts entities (dates, amounts, vendors, etc.)
6. Validates extracted data
7. Stores results in invoice_extractions table
8. Returns processing result
```

### Database Tables Used
```sql
-- Main extraction table (36 columns)
invoice_extractions (
    id, document_id, user_id,
    invoice_number, invoice_date, due_date,
    vendor_name, vendor_address, vendor_tax_id,
    customer_name, customer_address,
    total_amount, tax_amount, subtotal_amount,
    currency, payment_terms, line_items,
    confidence_score, processing_time,
    created_at, updated_at, ...
)
```

---

## ğŸ“Š Analytics Service (Port 8002)

**Service:** `src/microservices/analytics/main.py` (1,800+ lines)

### Endpoints
```
GET  /health                    â†’ Health check
GET  /health/live               â†’ Liveness probe
GET  /health/ready              â†’ Readiness probe
GET  /dashboard                 â†’ HTML dashboard
GET  /analytics/realtime        â†’ Real-time analytics
POST /analytics/historical      â†’ Historical data
GET  /metrics/performance       â†’ Performance metrics
GET  /analytics/business-intelligence â†’ BI insights
GET  /alerts                    â†’ Active alerts
POST /alerts/acknowledge/{id}   â†’ Acknowledge alert
POST /alerts/rules              â†’ Create alert rule
POST /analytics/store-metric    â†’ Store metric
GET  /analytics/metrics         â†’ Get metrics
POST /analytics/store-data-lake â†’ Store to data lake
GET  /analytics/data-lake/files â†’ List data lake files
GET  /analytics/processing-jobs/{user_id} â†’ User jobs
POST /analytics/powerbi/push-metrics â†’ Push to Power BI
POST /analytics/powerbi/push-user-activity â†’ Push activity
POST /analytics/powerbi/create-dataset â†’ Create dataset
GET  /monitoring/health         â†’ Monitoring health
GET  /monitoring/alerts         â†’ Monitoring alerts
POST /monitoring/alert-rules    â†’ Alert rules
GET  /analytics/automation-metrics â†’ Automation metrics â­
POST /analytics/automation-score â†’ Calculate automation score
POST /analytics/automation-score-batch â†’ Batch scoring
GET  /analytics/automation-insights â†’ Automation insights
GET  /analytics/automation-trend â†’ Automation trend
GET  /monitoring/metrics        â†’ System metrics
```

### Automation Scoring Engine
```python
class AutomationScoringEngine:
    def calculate_automation_score(self, document_id: str):
        # Fetch extraction results from invoice_extractions
        results = sql_service.execute_query(
            "SELECT * FROM invoice_extractions WHERE document_id = ?",
            (document_id,)
        )
        
        # Calculate automation score (0-100%)
        score = self._calculate_score(results)
        
        # Store automation score
        sql_service.execute_query(
            "INSERT INTO automation_scores (...) VALUES (...)",
            (document_id, score, ...)
        )
        
        return score
```

---

## ğŸ’¬ AI Chat Service (Port 8004)

**Service:** `src/microservices/ai-chat/main.py` (953 lines)

### Endpoints
```
POST /chat/message              â†’ Send chat message
GET  /chat/conversations        â†’ List conversations
GET  /chat/conversations/{id}/messages â†’ Get messages
DELETE /chat/conversations/{id} â†’ Delete conversation
GET  /health                    â†’ Health check
```

### Chat Flow
```
1. User sends message via frontend
2. API Gateway forwards to ai-chat:8004
3. AI Chat Service:
   - Retrieves conversation history from PostgreSQL
   - Adds user message to context
   - Calls OpenAI GPT-4 API
   - Stores messages in conversations table
4. Returns AI response to user
```

### Database Operations
```python
# Store conversation message
sql_service.store_conversation_message({
    "conversation_id": conversation_id,
    "user_id": user_id,
    "role": "user",
    "content": message,
    "timestamp": datetime.utcnow()
})

# Get conversation history
conversations = sql_service.get_user_conversations(user_id, limit=10)
```

---

## ğŸ”Œ MCP Server (Port 8012)

**Service:** `src/microservices/mcp-server/main.py` (1,228 lines)

### MCP Protocol Endpoints
```
GET  /health                    â†’ Health check
GET  /mcp/capabilities          â†’ MCP capabilities
GET  /mcp/tools                 â†’ List available tools
POST /mcp/tools/execute         â†’ Execute tool
GET  /mcp/resources             â†’ List resources
POST /mcp/resources/read        â†’ Read resource
POST /mcp/invoice/extract       â†’ Extract invoice data
POST /mcp/invoice/validate      â†’ Validate invoice
POST /mcp/document/classify     â†’ Classify document
GET  /mcp/metrics/automation    â†’ Automation metrics
POST /mcp/fine-tuning/create-job â†’ Create fine-tuning job
POST /mcp/m365/process-document â†’ Process M365 document
POST /mcp/initialize            â†’ Initialize MCP session
POST /mcp/tools/list            â†’ List tools (protocol)
POST /mcp/tools/call            â†’ Call tool (protocol)
POST /mcp/resources/list        â†’ List resources (protocol)
POST /mcp/resources/read        â†’ Read resource (protocol)
POST /mcp/auth/token            â†’ Get auth token
GET  /mcp/auth/test-tokens      â†’ Test tokens
GET  /mcp/auth/me               â†’ Get current user
GET  /mcp/rate-limits           â†’ Get rate limits
GET  /mcp/permissions           â†’ Get permissions
```

### MCP Service Orchestration
```python
# MCP tool execution orchestrates calls to other services
SERVICE_URLS = {
    "ai-processing": "http://ai-processing:8001",
    "data-quality": "http://data-quality:8006",
    "analytics": "http://analytics:8002",
    "document-ingestion": "http://document-ingestion:8000",
    "ai-chat": "http://ai-chat:8004"
}

# Example: Extract invoice via MCP tool
@app.post("/mcp/invoice/extract")
async def extract_invoice(document_id: str):
    # Call AI Processing service
    async with httpx.AsyncClient() as client:
        response = await client.post(
            f"{SERVICE_URLS['ai-processing']}/process",
            json={"document_id": document_id}
        )
    return response.json()
```

---

## âœ… Data Quality Service (Port 8006)

**Service:** `src/microservices/data-quality/main.py` (703 lines)

### Endpoints
```
GET  /health                    â†’ Health check
POST /validate/document         â†’ Validate document data
POST /validate/analytics        â†’ Validate analytics data
POST /quality/metrics           â†’ Quality metrics
POST /quality/report            â†’ Generate quality report
GET  /profile/{table_name}      â†’ Profile database table
GET  /quality/dashboard         â†’ Quality dashboard
GET  /quality/alerts            â†’ Quality alerts
```

### Data Validation Flow
```
1. Receives validation request
2. Queries data from PostgreSQL
3. Performs validation checks:
   - Completeness (missing fields)
   - Accuracy (data format)
   - Consistency (cross-field validation)
   - Timeliness (data freshness)
4. Calculates quality score
5. Stores quality metrics
6. Returns validation report
```

---

## ğŸ“¦ Batch Processor Service (Port 8007)

**Service:** `src/microservices/batch-processor/main.py` (349 lines)

### Endpoints
```
GET  /health                    â†’ Health check
GET  /pipelines                 â†’ List pipelines
GET  /pipelines/{name}          â†’ Get pipeline details
POST /pipelines/{name}/execute  â†’ Execute pipeline
GET  /executions/{id}           â†’ Get execution status
GET  /executions                â†’ List executions
POST /process/documents         â†’ Process documents batch
POST /process/analytics         â†’ Process analytics batch
POST /process/users             â†’ Process users batch
GET  /monitoring/dashboard      â†’ Monitoring dashboard
GET  /monitoring/health         â†’ Health status
```

### Batch Processing Pipelines
```
1. document-processing-pipeline
   - Processes multiple documents
   - Calls ai-processing for each
   - Aggregates results

2. analytics-aggregation-pipeline
   - Aggregates analytics data
   - Calculates metrics
   - Updates dashboards

3. data-quality-pipeline
   - Validates data quality
   - Generates reports
   - Triggers alerts
```

---

## ğŸ“š Data Catalog Service (Port 8008)

**Service:** `src/microservices/data-catalog/main.py` (480 lines)

### Endpoints
```
GET  /health                    â†’ Health check
POST /assets                    â†’ Register data asset
GET  /assets                    â†’ List assets
GET  /assets/{id}               â†’ Get asset details
POST /lineage/relationships     â†’ Create lineage relationship
GET  /lineage/{id}              â†’ Get asset lineage
GET  /lineage/flow/{source}/{target} â†’ Get lineage flow
POST /search                    â†’ Search assets
GET  /impact/{id}               â†’ Get impact analysis
GET  /dashboard/overview        â†’ Dashboard overview
GET  /dashboard/lineage-graph   â†’ Lineage graph
```

---

## ğŸ¯ Performance Dashboard (Port 8005)

**Service:** `src/microservices/performance-dashboard/main.py` (296 lines)

### Endpoints
```
GET  /                          â†’ HTML dashboard
GET  /api/system-metrics        â†’ System metrics
GET  /api/performance-summary   â†’ Performance summary
POST /api/optimize-memory       â†’ Optimize memory
GET  /api/cache-stats           â†’ Cache statistics
GET  /health                    â†’ Health check
GET  /api/health                â†’ API health check
```

---

## ğŸ” Shared Modules - How Services Communicate

### 1. Database Layer (`src/shared/storage/sql_service.py`)

All services use SQLService for PostgreSQL operations:

```python
from src.shared.storage.sql_service import SQLService

# Initialize in each service
sql_service = SQLService(config.sql_connection_string)

# Common operations
sql_service.execute_query(query, params)  # SELECT queries
sql_service.execute_non_query(query, params)  # INSERT/UPDATE
sql_service.store_document(document_data)
sql_service.get_user_documents(user_id, limit)
sql_service.store_processing_job(...)
sql_service.get_metrics(metric_name, hours)
```

**PostgreSQL Connection:**
```
Database: documentintelligence
Host: postgres (docker) or localhost
Port: 5432
User: admin
Password: admin123
```

### 2. Cache Layer (`src/shared/cache/redis_cache.py`)

```python
from src.shared.cache.redis_cache import RedisCache, cache_result

cache = RedisCache(redis_url="redis://redis:6379")

# Decorator for caching
@cache_result(ttl=300)
async def get_document(document_id: str):
    return sql_service.get_document(document_id)
```

### 3. Authentication (`src/shared/auth/jwt_auth.py`)

```python
from src.shared.auth.jwt_auth import create_token, verify_token

# Generate JWT token
token = create_token(user_id="user123", role="admin")

# Verify token
payload = verify_token(token)
```

### 4. Monitoring (`src/shared/monitoring/metrics.py`)

```python
from src.shared.monitoring.metrics import monitor_performance

@monitor_performance(threshold=2.0)
async def process_document(document_id: str):
    # Automatically tracks execution time
    pass
```

---

## ğŸ”„ Complete Request Flow Example

### Upload & Process Invoice

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 1: Frontend Upload                                            â”‚
â”‚ React App â†’ POST /documents/upload                                 â”‚
â”‚ Headers: Authorization: Bearer <JWT_TOKEN>                         â”‚
â”‚ Body: multipart/form-data (file)                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 2: API Gateway (Port 8003)                                    â”‚
â”‚ â€¢ Validates JWT token                                              â”‚
â”‚ â€¢ Checks rate limit (Redis)                                        â”‚
â”‚ â€¢ Forwards to document-ingestion:8000                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 3: Document Ingestion (Port 8000)                             â”‚
â”‚ â€¢ Validates file (size, type)                                      â”‚
â”‚ â€¢ Uploads to blob storage (Azure/Local)                            â”‚
â”‚ â€¢ Stores in PostgreSQL: documents table                            â”‚
â”‚ â€¢ Creates processing job                                           â”‚
â”‚ â€¢ Calls ai-processing:8001 via HTTP                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 4: AI Processing (Port 8001)                                  â”‚
â”‚ â€¢ Retrieves document from storage                                  â”‚
â”‚ â€¢ Extracts text (OCR if needed)                                    â”‚
â”‚ â€¢ Routes to GPT-4 / Form Recognizer                                â”‚
â”‚ â€¢ Extracts invoice entities:                                       â”‚
â”‚   - invoice_number, invoice_date, vendor_name                      â”‚
â”‚   - total_amount, tax_amount, line_items                           â”‚
â”‚ â€¢ Stores in PostgreSQL: invoice_extractions table (36 columns)     â”‚
â”‚ â€¢ Returns processing result                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 5: Analytics Calculation (Port 8002)                          â”‚
â”‚ â€¢ Automation engine calculates score                               â”‚
â”‚ â€¢ Queries invoice_extractions table                                â”‚
â”‚ â€¢ Calculates automation percentage                                 â”‚
â”‚ â€¢ Stores in automation_scores table                                â”‚
â”‚ â€¢ Updates dashboard metrics                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ STEP 6: Response to Frontend                                       â”‚
â”‚ Returns: {                                                          â”‚
â”‚   "document_id": "uuid-1234",                                      â”‚
â”‚   "status": "uploaded",                                            â”‚
â”‚   "message": "Document uploaded successfully",                     â”‚
â”‚   "entities_extracted": 25                                         â”‚
â”‚ }                                                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ—„ï¸ Database Schema

### Core Tables

```sql
-- Documents table
CREATE TABLE documents (
    id SERIAL PRIMARY KEY,
    document_id VARCHAR(100) UNIQUE NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    file_name VARCHAR(500) NOT NULL,
    file_size INTEGER,
    content_type VARCHAR(100),
    blob_path VARCHAR(1000),
    document_type VARCHAR(50),
    status VARCHAR(50),
    metadata JSONB,
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Invoice extractions table (36 columns)
CREATE TABLE invoice_extractions (
    id SERIAL PRIMARY KEY,
    document_id VARCHAR(100) NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    invoice_number VARCHAR(100),
    invoice_date DATE,
    due_date DATE,
    vendor_name VARCHAR(500),
    vendor_address TEXT,
    vendor_tax_id VARCHAR(100),
    vendor_phone VARCHAR(50),
    vendor_email VARCHAR(200),
    customer_name VARCHAR(500),
    customer_address TEXT,
    customer_tax_id VARCHAR(100),
    customer_phone VARCHAR(50),
    customer_email VARCHAR(200),
    total_amount DECIMAL(15,2),
    tax_amount DECIMAL(15,2),
    subtotal_amount DECIMAL(15,2),
    discount_amount DECIMAL(15,2),
    shipping_amount DECIMAL(15,2),
    currency VARCHAR(10),
    payment_terms VARCHAR(200),
    payment_method VARCHAR(100),
    purchase_order_number VARCHAR(100),
    bank_account VARCHAR(100),
    line_items JSONB,
    notes TEXT,
    confidence_score DECIMAL(5,2),
    processing_time DECIMAL(10,2),
    model_used VARCHAR(100),
    extraction_method VARCHAR(100),
    validation_status VARCHAR(50),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW(),
    processed_at TIMESTAMP,
    reviewed_at TIMESTAMP
);

-- Automation scores table
CREATE TABLE automation_scores (
    id SERIAL PRIMARY KEY,
    document_id VARCHAR(100) NOT NULL,
    score DECIMAL(5,2) NOT NULL,
    completeness DECIMAL(5,2),
    accuracy DECIMAL(5,2),
    confidence DECIMAL(5,2),
    calculated_at TIMESTAMP DEFAULT NOW()
);

-- Conversations table (AI Chat)
CREATE TABLE conversations (
    id SERIAL PRIMARY KEY,
    conversation_id VARCHAR(100) UNIQUE NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    title VARCHAR(500),
    created_at TIMESTAMP DEFAULT NOW(),
    updated_at TIMESTAMP DEFAULT NOW()
);

-- Messages table (AI Chat)
CREATE TABLE messages (
    id SERIAL PRIMARY KEY,
    message_id VARCHAR(100) UNIQUE NOT NULL,
    conversation_id VARCHAR(100) NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    role VARCHAR(20) NOT NULL,  -- user, assistant
    content TEXT NOT NULL,
    timestamp TIMESTAMP DEFAULT NOW(),
    FOREIGN KEY (conversation_id) REFERENCES conversations(conversation_id)
);

-- Processing jobs table
CREATE TABLE processing_jobs (
    id SERIAL PRIMARY KEY,
    job_id VARCHAR(100) UNIQUE NOT NULL,
    user_id VARCHAR(100) NOT NULL,
    document_id VARCHAR(100),
    status VARCHAR(50),
    progress INTEGER DEFAULT 0,
    started_at TIMESTAMP,
    completed_at TIMESTAMP,
    error_message TEXT,
    created_at TIMESTAMP DEFAULT NOW()
);
```

---

## ğŸ” Service-to-Service Communication Matrix

| From Service | To Service | Protocol | Endpoint | Purpose |
|-------------|-----------|----------|----------|---------|
| **API Gateway** | Document Ingestion | HTTP | POST /documents/upload | Forward upload |
| **API Gateway** | Analytics | HTTP | GET /analytics/automation-metrics | Get metrics |
| **API Gateway** | AI Chat | HTTP | POST /chat/message | Forward message |
| **Document Ingestion** | AI Processing | HTTP | POST /process | Trigger processing |
| **MCP Server** | AI Processing | HTTP | POST /process | Orchestrate processing |
| **MCP Server** | Data Quality | HTTP | POST /validate/document | Validate data |
| **MCP Server** | Analytics | HTTP | GET /analytics/metrics | Get analytics |
| **Batch Processor** | AI Processing | HTTP | POST /process | Batch processing |
| **All Services** | PostgreSQL | TCP | :5432 | Database operations |
| **All Services** | Redis | TCP | :6379 | Caching |

---

## ğŸ“ˆ Performance & Monitoring

### Prometheus Metrics
```yaml
# Exposed by all services
- http_requests_total
- http_request_duration_seconds
- http_requests_in_progress
- cache_hits_total
- cache_misses_total
- database_query_duration_seconds
- document_processing_duration_seconds
- ai_model_inference_duration_seconds
```

### Health Check Pattern
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "timestamp": datetime.utcnow().isoformat(),
        "service": "service-name",
        "version": "2.0.0",
        "checks": {
            "database": check_database(),
            "cache": check_redis(),
            "storage": check_storage()
        }
    }
```

---

## ğŸ”’ Security Architecture

### Authentication Flow
```
1. User login â†’ POST /auth/login
2. API Gateway validates credentials (PostgreSQL)
3. Returns JWT token (expires in 1 hour)
4. Client stores token in localStorage
5. All subsequent requests include: Authorization: Bearer <token>
6. API Gateway validates JWT on every request
7. Token refresh via POST /auth/refresh
```

### Rate Limiting
```python
# Redis-based rate limiting
@rate_limit(requests=100, window=60)  # 100 req/min
async def endpoint():
    pass

# Per-user limits
RATE_LIMITS = {
    "free": 10,      # 10 req/min
    "pro": 100,      # 100 req/min
    "enterprise": 1000  # 1000 req/min
}
```

---

## ğŸš€ Key Integration Points

### 1. **Document Upload Pipeline**
```
Frontend â†’ API Gateway â†’ Document Ingestion â†’ AI Processing â†’ Analytics
```

### 2. **Chat Pipeline**
```
Frontend â†’ API Gateway â†’ AI Chat â†’ OpenAI API â†’ PostgreSQL
```

### 3. **MCP Tool Execution**
```
Claude Desktop â†’ MCP Server â†’ Multiple Services â†’ Response
```

### 4. **Batch Processing**
```
Scheduler â†’ Batch Processor â†’ AI Processing â†’ Data Quality â†’ Analytics
```

### 5. **Analytics Dashboard**
```
Frontend â†’ API Gateway â†’ Analytics â†’ PostgreSQL â†’ Automation Engine
```

---

## ğŸ“Š Summary Statistics

| Metric | Count |
|--------|-------|
| **Total Microservices** | 14 |
| **Total API Endpoints** | 150+ |
| **Total Code Lines** | 11,629 (main.py files) |
| **Database Tables** | 15+ |
| **External APIs** | 3 (OpenAI, Azure Form Recognizer, Azure Storage) |
| **Frontend Pages** | 20+ |
| **Shared Modules** | 8 (auth, cache, storage, monitoring, etc.) |
| **Docker Services** | 18 (14 app + 4 infra) |
| **Total HTTP Calls** | Varies (100-1000+ per upload) |

---

## ğŸ¯ Critical Paths for Interview Demo

### 1. **Document Upload & Processing**
- Frontend uploads PDF â†’ API Gateway â†’ Document Ingestion
- Extracts 25+ entities â†’ Stores in PostgreSQL
- **Demo Point:** Show real-time entity extraction

### 2. **Automation Metrics**
- Analytics service calculates 90%+ automation
- **Demo Point:** Display automation dashboard

### 3. **MCP Protocol Integration**
- Claude Desktop executes MCP tools
- **Demo Point:** Run extract-invoice tool

### 4. **AI Chat Assistance**
- Ask questions about documents
- **Demo Point:** Query "What's the total amount on invoice X?"

---

## ğŸ”§ Local Development URLs

| Service | URL |
|---------|-----|
| Frontend | http://localhost:3001 |
| API Gateway | http://localhost:8003 |
| Document Ingestion | http://localhost:8000 |
| AI Processing | http://localhost:8001 |
| Analytics | http://localhost:8002 |
| AI Chat | http://localhost:8004 |
| Performance Dashboard | http://localhost:8005 |
| Data Quality | http://localhost:8006 |
| Batch Processor | http://localhost:8007 |
| Data Catalog | http://localhost:8008 |
| MCP Server | http://localhost:8012 |
| PostgreSQL | localhost:5432 |
| Redis | localhost:6379 |
| Grafana | http://localhost:3000 |
| Prometheus | http://localhost:9090 |

---

**Document Status:** âœ… Complete  
**Last Updated:** December 28, 2025  
**Author:** System Analysis Agent  
**Purpose:** Compello AS Interview Preparation
